{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5af6411",
   "metadata": {},
   "source": [
    "Tasks:\n",
    "#### 1. Data preparation\n",
    "\n",
    "- Download a translation dataset (pick a language pair) from https://www.manythings.org/anki/\n",
    "\n",
    "\n",
    "\n",
    "- Alternatively, if you prefer, download morphological segmentation data from http://turing.iimas.unam.mx/wix/static/resources/language_data.tar.bz2. You can choose which language you want to work with or even try combining them. This dataset will likely be a bit faster to train than the MT one above. Also, one trick that has been shown to work on this type of data is to add in random strings that map to themselves, in order to teach the decoder to output mostly the same characters as it sees in the input (with the addition of the morpheme boundary characters).\n",
    "\n",
    "\n",
    "\n",
    "- Create three .tsv files, one for each of train/dev/test partitions (if you use the MT data you will need to choose how to split the data, probably something like 70%/15%/15% would work). Once you have the data in this format, you smiply need to update the code in the \"Load Data\" section to load your data.\n",
    "\n",
    "\n",
    "- For the NMT dataset, you may need to update the tokenization function depending on your language(s).\n",
    "\n",
    "\n",
    "#### 2. Compare RNN Decoder (`Decoder` in the code) vs. RNN Decoder with Attention (`AttentionDecoder` in the code) \n",
    "- Read through the code for the Encoder, Attention, and the two Decoder classes. Make sure you have some understanding of what is going on before preceding.\n",
    "\n",
    "- Train model (for ~50-100 epochs? more if time permits...) using the \"Vanilla Decoder\", which is the default.\n",
    "\n",
    "\n",
    "- Make the necessary changes to the code (there should only be 2, there places are marked with a \"TODO\" comment) in order to run the same experiment with The AttentionDecoder.\n",
    "\n",
    "\n",
    "- Compare the results (eg the validation loss). Do you notice any difference? For now, just look at the validation loss.\n",
    "\n",
    "- Add to the `evaluate` function so that you also report a metric (you choose what metric).\n",
    "\n",
    "\n",
    "#### 3. (Bonus 1) Implement Teacher Forcing\n",
    "- Currently, in the `Seq2Seq` class's `forward()` method, there is a parameter called `teacher_forcing_ratio`, but we don't use it. \"Teacher forcing\" is a technique for training seq2seq models where, at each timestep, you give the decoder the correct output from the previous time step with some probability (instead of always feeding it the prediction from the previous time step, which could be wrong). Implement teacher forcing in this method. Assume `teacher_forcing_ratio` is a float between 0 and 1, and indicates the proportion of time we give the correct input to the decoder.\n",
    "\n",
    "#### 4. (Bonus 2: pobably more relevant for the morphological segmentation corpus) Compare with RNN Transducer\n",
    "- Train an RNN Transducer (from a few weeks ago) on the same data and compare the performances. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cff514f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7ee264",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "We will use utilities from the Pytorch package \"torchtext\" to easily load the data and batch it using buckets according to length (in order to minimize padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9399f959",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rpugh/miniconda3/envs/l645/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "char_tokenize = lambda s: s.split()\n",
    "SRC = Field(tokenize=char_tokenize, init_token='<sow>', eos_token='<eow>', lower=True)\n",
    "TGT = Field(tokenize=char_tokenize, init_token='<sow>', eos_token='<eow>', lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e3e75ad3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rpugh/miniconda3/envs/l645/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/rpugh/miniconda3/envs/l645/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# TODO: Update this cell to load the dataset you chose. Once you have your data in 3 tsv \n",
    "# files (one per train/dev/test), just update the path and the names of the files.\n",
    "#\n",
    "path_to_data = \"./\"\n",
    "\n",
    "train_data, val_data, test_data = TabularDataset.splits(\n",
    "        path=path_to_data, train='train_w_words_wixarika.tsv',\n",
    "        validation='wixarika_dev.tsv', test='wixarika_test.tsv', format='tsv',\n",
    "        fields=[('src', SRC), ('tgt', TGT)])\n",
    "\n",
    "\n",
    "SRC.build_vocab(train_data)\n",
    "TGT.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "774248b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rpugh/miniconda3/envs/l645/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# TODO: play with the batch size. Depending on your machine and dataset you may be able to get \n",
    "# away with much larger batches.\n",
    "#\n",
    "BATCH_SIZE = 24\n",
    "\n",
    "\n",
    "(train_iterator, valid_iterator, test_iterator) = BucketIterator.splits(\n",
    "    (train_data, val_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.src) # batch by length in order to minimize sequence padding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9b9dc6",
   "metadata": {},
   "source": [
    "## Define Model (Encoder, Decoder, Attention Layer, and Decoder with Attention)\n",
    "We define both a \"standard\" decoder and an attention decoder, so that we can evaluate the impact of attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d56d9d",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1684099d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):        \n",
    "        embedded = self.dropout(self.embedding(src))     \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6880040c",
   "metadata": {},
   "source": [
    "### Vanilla Decoder (no attention mechanism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4f96ce68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = vocab_size\n",
    "        self.hid_dim = dec_hid_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, dec_hid_dim)\n",
    "        self.fc_out = nn.Linear(dec_hid_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        #\n",
    "        # On the first time step, the hidden tensor \n",
    "        # (the context vector from the encoder) is only 2d, \n",
    "        # so we unsqueeze it.\n",
    "        #\n",
    "        if len(hidden.shape) == 2:\n",
    "            hidden = hidden.unsqueeze(0)\n",
    "            \n",
    "        input = input.unsqueeze(0)        \n",
    "        embedded = self.dropout(self.embedding(input))                \n",
    "        outputs, hidden = self.rnn(embedded, hidden)\n",
    "        prediction = self.fc_out(outputs.squeeze(0))        \n",
    "        return prediction, hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19896e5d",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "16c21381",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #\n",
    "        # Repeat decoder hidden state src_len times in order to concatenate it \n",
    "        # with the encoder outputs.\n",
    "        #\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        # energy shape: [batch size, src len, dec hid dim]\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2)))\n",
    "        # attention shape: [batch size, src len]\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ce296",
   "metadata": {},
   "source": [
    "### Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f864c368",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs) \n",
    "        a = a.unsqueeze(1)\n",
    "                \n",
    "        #\n",
    "        # Get weighted sum of encoder states (weighted by attention vector)\n",
    "        #\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "                \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)            \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #\n",
    "        # Also feed the input embedding and the attended encoder representation \n",
    "        # to the fully connected output layer.\n",
    "        #\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "                \n",
    "        return prediction, hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5d524e",
   "metadata": {},
   "source": [
    "## Putting it all together (the Seq2Seq model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0ce38f36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #\n",
    "            # TODO: change to accomodate the AttentionDecoder forward() call.\n",
    "            #\n",
    "            # answer: output, hidden = self.decoder(input, hidden, encoder_outputs) \n",
    "            output, hidden = self.decoder(input, hidden)  \n",
    "\n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            input = trg[t]\n",
    "            #\n",
    "            # TODO: (Bonus task) implement teacher forcing here\n",
    "            #\n",
    "            # Answer:\n",
    "            # if teacher_forcing_ratio > 0:\n",
    "            #     teacher_force = random.random() < teacher_forcing_ratio\n",
    "            #     input = trg[t] if teacher_force else top1\n",
    "            # else:\n",
    "            #     input = trg[t]\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee34bc2",
   "metadata": {},
   "source": [
    "## Training Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a3f99fc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(30, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(31, 256)\n",
       "    (rnn): GRU(256, 512)\n",
       "    (fc_out): Linear(in_features=512, out_features=31, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TGT.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "\n",
    "# \n",
    "# TODO:\n",
    "# The following line defines the decoder as a Vanilla RNN (GRU) Decoder (i.e. no attention). \n",
    "# Your task is to update this line to use the Bahdanau decoder (AttentionDecoder). You will\n",
    "# need to check out the __init__ method of AttentionDecoder to make sure you are passing it the\n",
    "# appropriate args.\n",
    "#\n",
    "# ANSWER: dec = AttentionDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, DEC_DROPOUT)\n",
    "\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b365ae00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    print(\"Starting training...\")\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.tgt\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, 0.5)  # use teacher forcing during training only.\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)        \n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4e53ea4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.tgt\n",
    "            output = model(src, trg, 0) # turn off teacher forcing   \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "83178a05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bac6e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch: 01 | Time: 0m 19s\n",
      "\tTrain Loss: 2.227\n",
      "\t Val. Loss: 1.371\n",
      "Starting training...\n",
      "Epoch: 02 | Time: 0m 20s\n",
      "\tTrain Loss: 1.736\n",
      "\t Val. Loss: 1.190\n",
      "Starting training...\n",
      "Epoch: 03 | Time: 0m 21s\n",
      "\tTrain Loss: 1.630\n",
      "\t Val. Loss: 1.290\n",
      "Starting training...\n",
      "Epoch: 04 | Time: 0m 21s\n",
      "\tTrain Loss: 1.661\n",
      "\t Val. Loss: 1.251\n",
      "Starting training...\n",
      "Epoch: 05 | Time: 0m 18s\n",
      "\tTrain Loss: 1.690\n",
      "\t Val. Loss: 1.223\n",
      "Starting training...\n",
      "Epoch: 06 | Time: 0m 19s\n",
      "\tTrain Loss: 1.522\n",
      "\t Val. Loss: 1.245\n",
      "Starting training...\n",
      "Epoch: 07 | Time: 0m 19s\n",
      "\tTrain Loss: 1.454\n",
      "\t Val. Loss: 1.054\n",
      "Starting training...\n",
      "Epoch: 08 | Time: 0m 19s\n",
      "\tTrain Loss: 1.262\n",
      "\t Val. Loss: 1.081\n",
      "Starting training...\n",
      "Epoch: 09 | Time: 0m 19s\n",
      "\tTrain Loss: 1.455\n",
      "\t Val. Loss: 1.512\n",
      "Starting training...\n",
      "Epoch: 10 | Time: 0m 19s\n",
      "\tTrain Loss: 1.888\n",
      "\t Val. Loss: 1.406\n",
      "Starting training...\n",
      "Epoch: 11 | Time: 0m 18s\n",
      "\tTrain Loss: 1.971\n",
      "\t Val. Loss: 1.541\n",
      "Starting training...\n",
      "Epoch: 12 | Time: 0m 18s\n",
      "\tTrain Loss: 2.180\n",
      "\t Val. Loss: 1.878\n",
      "Starting training...\n",
      "Epoch: 13 | Time: 0m 18s\n",
      "\tTrain Loss: 2.649\n",
      "\t Val. Loss: 2.095\n",
      "Starting training...\n",
      "Epoch: 14 | Time: 0m 18s\n",
      "\tTrain Loss: 2.846\n",
      "\t Val. Loss: 2.118\n",
      "Starting training...\n",
      "Epoch: 15 | Time: 0m 18s\n",
      "\tTrain Loss: 2.712\n",
      "\t Val. Loss: 1.868\n",
      "Starting training...\n",
      "Epoch: 16 | Time: 0m 19s\n",
      "\tTrain Loss: 2.546\n",
      "\t Val. Loss: 1.712\n",
      "Starting training...\n",
      "Epoch: 17 | Time: 0m 18s\n",
      "\tTrain Loss: 2.281\n",
      "\t Val. Loss: 1.674\n",
      "Starting training...\n",
      "Epoch: 18 | Time: 0m 18s\n",
      "\tTrain Loss: 2.336\n",
      "\t Val. Loss: 1.725\n",
      "Starting training...\n",
      "Epoch: 19 | Time: 0m 18s\n",
      "\tTrain Loss: 2.345\n",
      "\t Val. Loss: 1.595\n",
      "Starting training...\n",
      "Epoch: 20 | Time: 0m 19s\n",
      "\tTrain Loss: 2.311\n",
      "\t Val. Loss: 1.725\n",
      "Starting training...\n",
      "Epoch: 21 | Time: 0m 18s\n",
      "\tTrain Loss: 2.288\n",
      "\t Val. Loss: 1.606\n",
      "Starting training...\n",
      "Epoch: 22 | Time: 0m 18s\n",
      "\tTrain Loss: 2.269\n",
      "\t Val. Loss: 1.849\n",
      "Starting training...\n",
      "Epoch: 23 | Time: 0m 18s\n",
      "\tTrain Loss: 2.485\n",
      "\t Val. Loss: 1.821\n",
      "Starting training...\n",
      "Epoch: 24 | Time: 0m 18s\n",
      "\tTrain Loss: 2.604\n",
      "\t Val. Loss: 1.858\n",
      "Starting training...\n",
      "Epoch: 25 | Time: 0m 18s\n",
      "\tTrain Loss: 2.549\n",
      "\t Val. Loss: 1.813\n",
      "Starting training...\n",
      "Epoch: 26 | Time: 0m 18s\n",
      "\tTrain Loss: 2.502\n",
      "\t Val. Loss: 2.112\n",
      "Starting training...\n",
      "Epoch: 27 | Time: 0m 18s\n",
      "\tTrain Loss: 2.475\n",
      "\t Val. Loss: 1.783\n",
      "Starting training...\n",
      "Epoch: 28 | Time: 0m 18s\n",
      "\tTrain Loss: 2.452\n",
      "\t Val. Loss: 1.628\n",
      "Starting training...\n",
      "Epoch: 29 | Time: 0m 19s\n",
      "\tTrain Loss: 2.374\n",
      "\t Val. Loss: 1.669\n",
      "Starting training...\n",
      "Epoch: 30 | Time: 0m 18s\n",
      "\tTrain Loss: 2.265\n",
      "\t Val. Loss: 1.624\n",
      "Starting training...\n",
      "Epoch: 31 | Time: 0m 18s\n",
      "\tTrain Loss: 2.131\n",
      "\t Val. Loss: 1.494\n",
      "Starting training...\n",
      "Epoch: 32 | Time: 0m 18s\n",
      "\tTrain Loss: 2.156\n",
      "\t Val. Loss: 1.647\n",
      "Starting training...\n",
      "Epoch: 33 | Time: 0m 18s\n",
      "\tTrain Loss: 2.161\n",
      "\t Val. Loss: 1.729\n",
      "Starting training...\n",
      "Epoch: 34 | Time: 0m 19s\n",
      "\tTrain Loss: 2.203\n",
      "\t Val. Loss: 1.536\n",
      "Starting training...\n",
      "Epoch: 35 | Time: 0m 19s\n",
      "\tTrain Loss: 2.101\n",
      "\t Val. Loss: 1.659\n",
      "Starting training...\n",
      "Epoch: 36 | Time: 0m 18s\n",
      "\tTrain Loss: 2.054\n",
      "\t Val. Loss: 1.475\n",
      "Starting training...\n",
      "Epoch: 37 | Time: 0m 18s\n",
      "\tTrain Loss: 2.047\n",
      "\t Val. Loss: 1.580\n",
      "Starting training...\n",
      "Epoch: 38 | Time: 0m 19s\n",
      "\tTrain Loss: 2.036\n",
      "\t Val. Loss: 1.539\n",
      "Starting training...\n",
      "Epoch: 39 | Time: 0m 19s\n",
      "\tTrain Loss: 1.992\n",
      "\t Val. Loss: 1.482\n",
      "Starting training...\n",
      "Epoch: 40 | Time: 0m 19s\n",
      "\tTrain Loss: 2.023\n",
      "\t Val. Loss: 1.698\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 150\n",
    "CLIP = 1\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "TRG_PAD_IDX = TGT.vocab.stoi[TGT.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut3-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bda85f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k e n e u p i t + a\tk e ! n ! e u ! p i ! t + a\r\n",
      "p a p a t +\tp a ! p a ! t +\r\n",
      "m a y e m a\tm ! a ! y e ! m a\r\n",
      "p e m +\tp ! e ! m +\r\n",
      "w i k + a y a t +\tw i k + a y a ! t +\r\n",
      "m a t s i ' u t a x a n e t a x +\tm a ! t s i ' ! u ! t a ! x a n e t a ! x +\r\n",
      "n e p i t i t u a n i\tn e ! p ! i ! t i ! t u a ! n i\r\n",
      "n e x a w e r i\tn e ! x a w e r i\r\n",
      "p u k a w e\tp ! u ! k a ! w e\r\n",
      "k u r a r u\tk u r a r u\r\n"
     ]
    }
   ],
   "source": [
    "! head wixarika_dev.tsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cc8738ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.tgt\n",
    "        output = model(src, trg, 0) # turn off teacher forcing  \n",
    "        break\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "47ec710c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21338155515370705"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ismatch(outputseq, targetseq):\n",
    "    for i, char in enumerate(outputseq):\n",
    "        if char == \"<eow>\" and targetseq[i] == char:\n",
    "            return True\n",
    "        if char != targetseq[i]:\n",
    "            return False\n",
    "        \n",
    "def accuracy(iterator):\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.tgt\n",
    "        output = model(src, trg, 0) # turn off teacher forcing \n",
    "        predseqs, targetseqs = [], []\n",
    "        for pred in output[1:].argmax(2).transpose(1, 0):\n",
    "            predseqs.append([TGT.vocab.itos[i.item()] for i in pred])\n",
    "        for pred in trg[1:].transpose(1, 0):\n",
    "            targetseqs.append([TGT.vocab.itos[i.item()] for i in pred])\n",
    "        \n",
    "        for i, seq in enumerate(predseqs):\n",
    "            if ismatch(seq, targetseqs[i]):\n",
    "                correct += 1\n",
    "                total += 1\n",
    "            else:\n",
    "                total += 1\n",
    "    return correct / total\n",
    "                    \n",
    "accuracy(test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "be4a1d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m', '!', 'm', 'e', '<eow>', '<eow>', '<eow>']\n",
      "['x', '+', 'k', 'e', '<eow>', '<eow>', '<eow>']\n",
      "[\"'\", 'u', 't', 't', 'a', '<eow>', '<eow>']\n",
      "['p', 'a', 'a', '!', 'k', 'a', '<eow>']\n",
      "['k', 'i', 'y', 'y', 'a', '<eow>', '<eow>']\n",
      "['p', '!', '!', 'k', 'a', '<eow>', '<eow>']\n",
      "['n', 'a', 'w', 'a', '<eow>', '<eow>', '<eow>']\n",
      "['w', 'i', 'k', 'k', 'i', '<eow>', '<eow>']\n",
      "['m', 'a', 'n', 'a', '<eow>', '<eow>', '<eow>']\n",
      "['m', '+', 'y', 'y', '+', '<eow>', '<eow>']\n",
      "['m', '+', '<eow>', '<eow>', '<eow>', '<eow>', '<eow>']\n",
      "['u', 'w', 'a', '<eow>', '<eow>', '<eow>', '<eow>']\n",
      "['e', 'n', 'a', '<eow>', '<eow>', '<eow>', '<eow>']\n",
      "['i', 'k', 'i', '<eow>', '<eow>', '<eow>', '<eow>']\n",
      "['u', 'm', 'a', '<eow>', '<eow>', '<eow>', '<eow>']\n",
      "['i', 'y', 'a', '<eow>', '<eow>', '<eow>', '<eow>']\n",
      "['u', 't', 't', 'a', '<eow>', '<eow>', '<eow>']\n",
      "['u', 'k', 'i', '<eow>', '<eow>', '<eow>', '<eow>']\n",
      "['w', 'a', 'i', '<eow>', '<eow>', '<eow>', '<eow>']\n",
      "['x', 'e', 'i', '<eow>', '<eow>', '<eow>', '<eow>']\n",
      "['h', '+', '<eow>', '<eow>', '<eow>', '<eow>', '<eow>']\n",
      "['j', 'a', '<eow>', '<eow>', '<eow>', '<eow>', '<eow>']\n",
      "['y', 'e', '<eow>', '<eow>', '<eow>', '<eow>', '<eow>']\n",
      "['n', 'e', '<eow>', '<eow>', '<eow>', '<eow>', '<eow>']\n"
     ]
    }
   ],
   "source": [
    "for pred in output[1:].argmax(2).transpose(1, 0):\n",
    "    print([TGT.vocab.itos[i.item()] for i in pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6e47dc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m', 'i', 'm', 'e', '<eow>', '<pad>', '<pad>']\n",
      "['x', '+', 'y', 'e', '<eow>', '<pad>', '<pad>']\n",
      "[\"'\", 'u', '!', 't', 'a', '<eow>', '<pad>']\n",
      "['p', '!', 'a', '!', 'k', 'a', '<eow>']\n",
      "['k', 'i', '!', 'y', 'a', '<eow>', '<pad>']\n",
      "['p', 'e', '!', 'k', 'a', '<eow>', '<pad>']\n",
      "['n', 'a', 'w', 'i', '<eow>', '<pad>', '<pad>']\n",
      "['w', 'i', '!', 'k', 'i', '<eow>', '<pad>']\n",
      "['m', 'a', 'n', 'a', '<eow>', '<pad>', '<pad>']\n",
      "['m', '+', '!', 'y', 'e', '<eow>', '<pad>']\n",
      "['m', '+', 'k', '<eow>', '<pad>', '<pad>', '<pad>']\n",
      "['u', 'w', 'a', '<eow>', '<pad>', '<pad>', '<pad>']\n",
      "['e', 'n', 'a', '<eow>', '<pad>', '<pad>', '<pad>']\n",
      "['i', 'k', 'i', '<eow>', '<pad>', '<pad>', '<pad>']\n",
      "['u', 'm', 'a', '<eow>', '<pad>', '<pad>', '<pad>']\n",
      "['i', 'y', 'a', '<eow>', '<pad>', '<pad>', '<pad>']\n",
      "['u', '!', 't', 'a', '<eow>', '<pad>', '<pad>']\n",
      "['u', 'k', 'i', '<eow>', '<pad>', '<pad>', '<pad>']\n",
      "['w', 'a', 'i', '<eow>', '<pad>', '<pad>', '<pad>']\n",
      "['x', 'e', 'i', '<eow>', '<pad>', '<pad>', '<pad>']\n",
      "['h', '+', '<eow>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['j', 'a', '<eow>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['y', 'e', '<eow>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['n', 'e', '<eow>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "for pred in trg[1:].transpose(1, 0):\n",
    "    print([TGT.vocab.itos[i.item()] for i in pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "990e1ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   864 TESTALLWORDS.tsv\r\n",
      "  1664 train_w_words_wixarika.tsv\r\n",
      "   167 wixarika_dev.tsv\r\n",
      "   553 wixarika_test.tsv\r\n",
      "   665 wixarika_train.tsv\r\n",
      "  3913 total\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l *tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a742f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
