{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5af6411",
   "metadata": {
    "id": "c5af6411"
   },
   "source": [
    "Tasks:\n",
    "#### 1. Data preparation\n",
    "\n",
    "- Download a translation dataset (pick a language pair) from https://www.manythings.org/anki/\n",
    "\n",
    "\n",
    "\n",
    "- Alternatively, if you prefer, download morphological segmentation data from http://turing.iimas.unam.mx/wix/static/resources/language_data.tar.bz2. You can choose which language you want to work with or even try combining them. This dataset will likely be a bit faster to train than the MT one above. Also, one trick that has been shown to work on this type of data is to add in random strings that map to themselves, in order to teach the decoder to output mostly the same characters as it sees in the input (with the addition of the morpheme boundary characters).\n",
    "\n",
    "\n",
    "\n",
    "- Create three .tsv files, one for each of train/dev/test partitions (if you use the MT data you will need to choose how to split the data, probably something like 70%/15%/15% would work). Once you have the data in this format, you smiply need to update the code in the \"Load Data\" section to load your data.\n",
    "\n",
    "\n",
    "- For the NMT dataset, you may need to update the tokenization function depending on your language(s).\n",
    "\n",
    "\n",
    "#### 2. Compare RNN Decoder (`Decoder` in the code) vs. RNN Decoder with Attention (`AttentionDecoder` in the code) \n",
    "- Read through the code for the Encoder, Attention, and the two Decoder classes. Make sure you have some understanding of what is going on before preceding.\n",
    "\n",
    "- Train model (for ~50-100 epochs? more if time permits...) using the \"Vanilla Decoder\", which is the default.\n",
    "\n",
    "\n",
    "- Make the necessary changes to the code (there should only be 2, there places are marked with a \"TODO\" comment) in order to run the same experiment with The AttentionDecoder.\n",
    "\n",
    "\n",
    "- Compare the results (eg the validation loss). Do you notice any difference? For now, just look at the validation loss.\n",
    "\n",
    "\n",
    "- Add to the `evaluate` function so that you also report a metric (you choose what metric). Alternatively for an easier task, complete the bleu-score function towards the end of the notebook.\n",
    "\n",
    "\n",
    "#### 3. (Bonus 1) Implement Teacher Forcing\n",
    "- Currently, in the `Seq2Seq` class's `forward()` method, there is a parameter called `teacher_forcing_ratio`, but we don't use it. \"Teacher forcing\" is a technique for training seq2seq models where, at each timestep, you give the decoder the correct output from the previous time step with some probability (instead of always feeding it the prediction from the previous time step, which might be wrong). Implement teacher forcing in this method. Assume `teacher_forcing_ratio` is a float between 0 and 1, and indicates the proportion of time we give the correct input to the decoder.\n",
    "\n",
    "#### 4. (Bonus 2: pobably more relevant for the morphological segmentation corpus) Compare with RNN Transducer\n",
    "- Train an RNN Transducer (from a few weeks ago) on the same data and compare the performances. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Us3Dn2sWQPal",
   "metadata": {
    "id": "Us3Dn2sWQPal"
   },
   "source": [
    "<hr>\n",
    "\n",
    "### NOTES:\n",
    "\n",
    "To begin with...\n",
    "\n",
    "1. If there is any import Error for Fields from torchtext.data, we would then need specific version of torchtext (0.8), please pip install requirements.txt which is present.\n",
    "\n",
    "To pip install from colab notebook cell-\n",
    "\n",
    "```\n",
    "!pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "\n",
    "2. ENG-DEU file download and train, val, test file creation-\n",
    "\n",
    "```\n",
    "!python eng_german_sample_download.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SLA3dpG-ZHNW",
   "metadata": {
    "id": "SLA3dpG-ZHNW"
   },
   "source": [
    "If there is OSError: ... so: undefined symbol: ... , please re-execute the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff514f",
   "metadata": {
    "id": "9cff514f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7ee264",
   "metadata": {
    "id": "de7ee264"
   },
   "source": [
    "## Load Data\n",
    "We will use utilities from the Pytorch package \"torchtext\" to easily load the data and batch it using buckets according to length (in order to minimize padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9399f959",
   "metadata": {
    "id": "9399f959",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "char_tokenize = lambda s: s.split()\n",
    "SRC = Field(tokenize=char_tokenize, init_token='<sow>', eos_token='<eow>', lower=True)\n",
    "TGT = Field(tokenize=char_tokenize, init_token='<sow>', eos_token='<eow>', lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e75ad3",
   "metadata": {
    "id": "e3e75ad3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# TODO: Update this cell to load the dataset you chose. Once you have your data in 3 tsv \n",
    "# files (one per train/dev/test), just update the path and the names of the files.\n",
    "#\n",
    "path_to_data = \"../LING-L645/\" # <give your data folder location here.>\n",
    "\n",
    "train_data, val_data, test_data = TabularDataset.splits(\n",
    "        path=path_to_data, train='wixarika_train.tsv', # <your train.tsv file here>\n",
    "        validation='wixarika_dev.tsv', # <your val.tsv file here>\n",
    "        test='wixarika_test.tsv', # <your test.tsv file here>\n",
    "        format='tsv',\n",
    "        fields=[('src', SRC), ('tgt', TGT)])\n",
    "\n",
    "# If your dataset is huge and contains many unique words, then for the sake of fast execution, you can add this argument: min_freq = <some integer>\n",
    "# Only tokens that appear atleast <some integer> times then are considered. Other such words are replaced by < UNK >\n",
    "\n",
    "SRC.build_vocab(train_data)\n",
    "TGT.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g31X-2WJTavn",
   "metadata": {
    "id": "g31X-2WJTavn"
   },
   "outputs": [],
   "source": [
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TGT.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774248b1",
   "metadata": {
    "id": "774248b1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# TODO: play with the batch size. Depending on your machine and dataset you may be able to get #\n",
    "# away with much larger batches.                                                               #\n",
    "################################################################################################\n",
    "BATCH_SIZE = 8 \n",
    "\n",
    "(train_iterator, valid_iterator, test_iterator) = BucketIterator.splits(\n",
    "    (train_data, val_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.src) # batch by length in order to minimize sequence padding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9b9dc6",
   "metadata": {
    "id": "7e9b9dc6"
   },
   "source": [
    "## Define Model (Encoder, Decoder, Attention Layer, and Decoder with Attention)\n",
    "We define both a \"standard\" decoder and an attention decoder, so that we can evaluate the impact of attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d56d9d",
   "metadata": {
    "id": "f1d56d9d"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1684099d",
   "metadata": {
    "id": "1684099d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):        \n",
    "        embedded = self.dropout(self.embedding(src))     \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6880040c",
   "metadata": {
    "id": "6880040c"
   },
   "source": [
    "### Vanilla Decoder (no attention mechanism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96ce68",
   "metadata": {
    "id": "4f96ce68",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = vocab_size\n",
    "        self.hid_dim = dec_hid_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, dec_hid_dim)\n",
    "        self.fc_out = nn.Linear(dec_hid_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        #\n",
    "        # On the first time step, the hidden tensor \n",
    "        # (the context vector from the encoder) is only 2d, \n",
    "        # so we unsqueeze it.\n",
    "        #\n",
    "        if len(hidden.shape) == 2:\n",
    "            hidden = hidden.unsqueeze(0)\n",
    "            \n",
    "        input = input.unsqueeze(0)        \n",
    "        embedded = self.dropout(self.embedding(input))                \n",
    "        outputs, hidden = self.rnn(embedded, hidden)\n",
    "        prediction = self.fc_out(outputs.squeeze(0)) \n",
    "        # last value returned is for attention. Since vanilla model doesn't have attention, this value is None.       \n",
    "        return prediction, hidden, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19896e5d",
   "metadata": {
    "id": "19896e5d"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c21381",
   "metadata": {
    "id": "16c21381",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #\n",
    "        # Repeat decoder hidden state src_len times in order to concatenate it \n",
    "        # with the encoder outputs.\n",
    "        #\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        # energy shape: [batch size, src len, dec hid dim]\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2)))\n",
    "        # attention shape: [batch size, src len]\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ce296",
   "metadata": {
    "id": "f23ce296"
   },
   "source": [
    "### Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f864c368",
   "metadata": {
    "id": "f864c368",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs) \n",
    "        a = a.unsqueeze(1)\n",
    "                \n",
    "        #\n",
    "        # Get weighted sum of encoder states (weighted by attention vector)\n",
    "        #\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "                \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)            \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #\n",
    "        # Also feed the input embedding and the attended encoder representation \n",
    "        # to the fully connected output layer.\n",
    "        #\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "                \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5d524e",
   "metadata": {
    "id": "9c5d524e"
   },
   "source": [
    "## Putting it all together (the Seq2Seq model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce38f36",
   "metadata": {
    "id": "0ce38f36",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            ################################################################## #\n",
    "            # TODO*: change to accomodate the AttentionDecoder forward() call.  #\n",
    "            #       You will also need to change a line in the next cell (look # \n",
    "            #       for asterisk)\n",
    "            ##################################################################\n",
    "            output, hidden, _ = self.decoder(input, hidden)  \n",
    "\n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "\n",
    "            #################################################################\n",
    "            # TODO: (Bonus task) implement teacher forcing here             #\n",
    "            #################################################################\n",
    "            input = trg[t]\n",
    "\n",
    "\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee34bc2",
   "metadata": {
    "id": "1ee34bc2"
   },
   "source": [
    "## Training Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f99fc0",
   "metadata": {
    "id": "a3f99fc0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TGT.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "\n",
    "###################################################################################################\n",
    "# TODO*:                                                                                           #\n",
    "# The following line defines the decoder as a Vanilla RNN (GRU) Decoder (i.e. no attention).      #\n",
    "# Your task is to update this line to use the Bahdanau decoder (AttentionDecoder). You will       #\n",
    "# need to check out the __init__ method of AttentionDecoder to make sure you are passing it the   #\n",
    "# appropriate args.                                                                               #\n",
    "###################################################################################################\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, DEC_DROPOUT)\n",
    "\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365ae00",
   "metadata": {
    "id": "b365ae00",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    print(\"Starting training...\")\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.tgt\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, 0.5)  # use teacher forcing during training only.\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)        \n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e53ea4e",
   "metadata": {
    "id": "4e53ea4e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.tgt\n",
    "            output = model(src, trg, 0) # turn off teacher forcing   \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83178a05",
   "metadata": {
    "id": "83178a05",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bac6e4",
   "metadata": {
    "id": "71bac6e4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 150\n",
    "CLIP = 1\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "TRG_PAD_IDX = TGT.vocab.stoi[TGT.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut3-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i18FFIWhVd63",
   "metadata": {
    "id": "i18FFIWhVd63"
   },
   "source": [
    "## Inference\n",
    "\n",
    "The following blocks are only valid for attention model. If you are executing Vanilla Decoder, then stop here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PRpSSrIZV5mN",
   "metadata": {
    "id": "PRpSSrIZV5mN"
   },
   "source": [
    "We are now going to machine translate some sentences using our trained attention model.\n",
    "\n",
    "The function takes in sentence to translate, SRC, TGT fields and the model and returns predicted sentence and attention values over the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7nUL2YW9V6SR",
   "metadata": {
    "id": "7nUL2YW9V6SR"
   },
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50):\n",
    "    model.eval()\n",
    "\n",
    "    tokens = [token.lower() for token in sentence]\n",
    "    # Add start of sentence and end of sentence to the tokens\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "\n",
    "    # Numericalize the source sentence\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    # src_indexes shape = [src_len]\n",
    "\n",
    "    # Convert to tensor and add batch dimension.\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
    "\n",
    "    # Feed source sentence into encoder\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "    \n",
    "    # create a list to hold the output sentence, initialized with an <sos> token\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    # create a tensor to hold the attention values\n",
    "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
    "\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        with torch.no_grad():\n",
    "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
    "        attentions[i] = attention\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "\n",
    "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GRzoKw1ZX0_O",
   "metadata": {
    "id": "GRzoKw1ZX0_O"
   },
   "source": [
    "Function to display attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m3gNyABdX1eK",
   "metadata": {
    "id": "m3gNyABdX1eK"
   },
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=15)\n",
    "    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
    "                       rotation=45)\n",
    "    ax.set_yticklabels(['']+translation)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jKyChZE9X9uV",
   "metadata": {
    "id": "jKyChZE9X9uV"
   },
   "source": [
    "### Inference on some test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UIRzF9X1YBDm",
   "metadata": {
    "id": "UIRzF9X1YBDm"
   },
   "outputs": [],
   "source": [
    "example_idx = 16\n",
    "\n",
    "src = vars(train_data.examples[example_idx])['src']\n",
    "trg = vars(train_data.examples[example_idx])['tgt']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j88NiyfwYO_I",
   "metadata": {
    "id": "j88NiyfwYO_I"
   },
   "outputs": [],
   "source": [
    "translation, attention = translate_sentence(src, SRC, TGT, model, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sdnJMH-DYScv",
   "metadata": {
    "id": "sdnJMH-DYScv"
   },
   "outputs": [],
   "source": [
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QSw6F_EhYWQl",
   "metadata": {
    "id": "QSw6F_EhYWQl"
   },
   "source": [
    "### Bleu score calculation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NkpXBUw2YZkk",
   "metadata": {
    "id": "NkpXBUw2YZkk"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for datum in data:\n",
    "        \n",
    "        src = vars(datum)['src']\n",
    "        tgt = vars(datum)['tgt']\n",
    "        \n",
    "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
    "        \n",
    "        #cut off <eos> token\n",
    "        pred_trg = pred_trg[:-1]\n",
    "        \n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([tgt])\n",
    "        \n",
    "        ################################################################\n",
    "        # TODO: call the bleu score function here and return the result#\n",
    "        ################################################################\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W42BqwGRYdTw",
   "metadata": {
    "id": "W42BqwGRYdTw"
   },
   "outputs": [],
   "source": [
    "bleu_score = calculate_bleu(test_data, SRC, TGT, model, device)\n",
    "\n",
    "print(f'BLEU score = {bleu_score*100:.2f}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
